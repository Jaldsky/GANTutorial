{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtSzxWV75aSB"
      },
      "source": [
        "# Подготовка"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUr3i3Z25aSC"
      },
      "source": [
        "## Конфигруация ширины\n",
        "\n",
        "Настраиваем ширину вывода таблиц Numpy и Pandas, чтобы они не переходили на новую строку без надобности, но помещались на экран."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQxbUTnU5aSD"
      },
      "outputs": [],
      "source": [
        "# Сколько символов поместится в ваш блокнот?\n",
        "width = 100\n",
        "print('A'*width)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrufZTge5aSD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(linewidth=width)\n",
        "\n",
        "np.arange(width)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e76CrS8t5aSE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.options.display.width = width - 10\n",
        "pd.options.display.max_colwidth = width - 10\n",
        "pd.DataFrame([{'col': 'A' * (width-11)}])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-eIy_iR5aSE"
      },
      "source": [
        "## Установка"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3ecIUyq5aSE"
      },
      "outputs": [],
      "source": [
        "# в colab можно ставить на систему, в jhub.jinr.ru - не разрешит\n",
        "!apt-get install -q openjdk-8-jdk-headless"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkm4qD-l5aSE"
      },
      "outputs": [],
      "source": [
        "# качаем спарк\n",
        "!wget -c https://apache-mirror.rbc.ru/pub/apache/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ha74vXZ55aSF"
      },
      "outputs": [],
      "source": [
        "# если ещё не извлекали - разархивируем\n",
        "!if [ ! -d spark-3.2.1-bin-hadoop3.2 ]; then tar -xzf spark-3.2.1-bin-hadoop3.2.tgz; fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2Jpb2gr5aSF"
      },
      "outputs": [],
      "source": [
        "pip install findspark # в jhub: pip install --user findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urKSWvaO5aSF"
      },
      "source": [
        "## Запуск"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ftjgx5Q15aSG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# чтобы спарк мог запускаться\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# чтобы скрипт нашёл библиотеки спарка\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "# в jhub:\n",
        "# os.environ[\"JAVA_HOME\"] = \"/zfs/store5.hydra.local/user/i/ikadochn/java_v8u181\"\n",
        "# os.environ[\"SPARK_HOME\"] = f\"{os.getcwd()}/spark-3.2.1-bin-hadoop3.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KX4B68uE5aSG"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROvxg5hu5aSG"
      },
      "outputs": [],
      "source": [
        "# сессия нового (SQL-подобного) API к Spark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[4]\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KQsSD6e5aSG"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "# а можно так\n",
        "conf = pyspark.conf.SparkConf()\n",
        "conf.setMaster(\"local[4]\")\n",
        "sc = pyspark.SparkContext.getOrCreate(conf)\n",
        "sc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBRWJCLb5aSK"
      },
      "source": [
        "# Spark DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXArE9X05aSK"
      },
      "source": [
        "## Создание из локальных данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IR_xeaMh5aSM"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    Row(a=1, b=2., c='string1'),\n",
        "    Row(a=2, b=3., c='string2'),\n",
        "    Row(a=4, b=5., c='string3')\n",
        "])\n",
        "\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2lneBZp5aSM"
      },
      "outputs": [],
      "source": [
        "df.explain()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAIT9lbS5aSM"
      },
      "outputs": [],
      "source": [
        "df.explain(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StCOGtQV5aSM"
      },
      "outputs": [],
      "source": [
        "help(df.explain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vENxQjbd5aSM"
      },
      "outputs": [],
      "source": [
        "df = spark.createDataFrame([\n",
        "    (1, 2., 'string1'),\n",
        "    (2, 3., 'string2'),\n",
        "    (3, 4., 'string3')\n",
        "], schema='a long, b double, c string')\n",
        "\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLS8g1XO5aSM"
      },
      "outputs": [],
      "source": [
        "df.select(\"a\").explain(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpp-7SYg5aSM"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  df = spark.createDataFrame({\n",
        "    'a': [1, 2, 3],\n",
        "    'b': [2., 3., 4.],\n",
        "    'c': ['string1', 'string2', 'string3'],\n",
        "  })\n",
        "  df.show()\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJrAIfz55aSN"
      },
      "outputs": [],
      "source": [
        "pandas_df = pd.DataFrame({\n",
        "    'a': [1, 2, 3],\n",
        "    'b': [2., 3., 4.],\n",
        "    'c': ['string1', 'string2', 'string3'],\n",
        "})\n",
        "df = spark.createDataFrame(pandas_df)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m8mVwf45aSN"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjcmVyYU5aSN"
      },
      "outputs": [],
      "source": [
        "sc.parallelize([1,2,3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hShi0G_5aSN"
      },
      "outputs": [],
      "source": [
        "rdd = sc.parallelize([\n",
        "    (1, 2., 'string1'),\n",
        "    (2, 3., 'string2'),\n",
        "    (3, 4., 'string3')\n",
        "])\n",
        "df = spark.createDataFrame(rdd, schema=['a', 'b', 'c'])\n",
        "df.show()\n",
        "rdd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iinqxyFu5aSN"
      },
      "source": [
        "## Просмотр, схема, сбор на клиенте"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXQAr2_Q5aSN"
      },
      "outputs": [],
      "source": [
        "df.show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmDYz2f-5aSN"
      },
      "outputs": [],
      "source": [
        "df.show(2, vertical=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VghCV8xx5aSN"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qoa_gASP5aSO"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dB0FK9i5aSO"
      },
      "outputs": [],
      "source": [
        "df.schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FVzP5bp5aSO"
      },
      "outputs": [],
      "source": [
        "df.take(2), df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92wT-0Sx5aSO"
      },
      "outputs": [],
      "source": [
        "df.rdd.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkCDF6et5aSO"
      },
      "outputs": [],
      "source": [
        "df.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGwPXJ8c5aSO"
      },
      "outputs": [],
      "source": [
        "df.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGEym7Oy5aSP"
      },
      "outputs": [],
      "source": [
        "df.printSchema()\n",
        "df.toPandas().info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZ7WBphk5aSP"
      },
      "outputs": [],
      "source": [
        "df.toPandas().describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PAX3-Yp5aSP"
      },
      "outputs": [],
      "source": [
        "df.describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0B9oUcTh5aSP"
      },
      "outputs": [],
      "source": [
        "df.summary().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJaXvgIo5aSP"
      },
      "source": [
        "# SQL?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y82e2-fs5aSP"
      },
      "source": [
        "## Столбцы датафрейма"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10WIrnXi5aSP"
      },
      "outputs": [],
      "source": [
        "df.select('a', 'c').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMckvt885aSP"
      },
      "outputs": [],
      "source": [
        "# df.filter('a'<3).show()\n",
        "df.filter(df.a < 3).show()\n",
        "df.filter(2 * df.a == df.b).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "202kiVoQ5aSP"
      },
      "outputs": [],
      "source": [
        "df.a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egtkqQES5aSQ"
      },
      "outputs": [],
      "source": [
        "df.select(df.a, df.c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVK1qmT85aSQ"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import upper\n",
        "\n",
        "df.withColumn('upper_c', upper(df.c)).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vWuIb_d5aSQ"
      },
      "outputs": [],
      "source": [
        "df.select(df.a * df.b).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sv5JSY225aSQ"
      },
      "outputs": [],
      "source": [
        "qq = spark.createDataFrame([\n",
        "    ('x', 'a'),\n",
        "    ('y', 'b'),\n",
        "], schema='a string, z string')\n",
        "try:\n",
        "  df.select(qq.a)\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kT59eNZr5aSQ"
      },
      "outputs": [],
      "source": [
        "q = df.select(df.a.alias('a1'), df.a.alias('a2'), 'c')\n",
        "q.show()\n",
        "q.filter(q.a1<3).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1oawWXV5aSQ"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "col('q')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68fwcclu5aSQ"
      },
      "outputs": [],
      "source": [
        "df.select(col('a')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYzAeNca5aSR"
      },
      "outputs": [],
      "source": [
        "df.select(df.a.alias('a1'), df.a.alias('a2'), 'c').where(col('a1')<3).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moKhXAhV5aSR"
      },
      "outputs": [],
      "source": [
        "df.filter(col('b') < 'a').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMFgqk4S5aSR"
      },
      "source": [
        "## Юнион (конкатенация по строкам)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuNaGDSV5aSR"
      },
      "outputs": [],
      "source": [
        "train = pd.DataFrame({\"x1\": range(3),\n",
        "                   \"y\": [\"a\", \"b\", \"a\"],\n",
        "                   \"x2\": reversed(range(3)),\n",
        "                   \"x3\": 0})\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W80_6nq-5aSR"
      },
      "outputs": [],
      "source": [
        "test = pd.DataFrame({\"x1\": [2, 3],\n",
        "                   \"x2\": [1, 2],\n",
        "                   \"x3\": 0})\n",
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OzTkhcz5aSS"
      },
      "outputs": [],
      "source": [
        "pd.concat([train, test], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnk3R7405aSS"
      },
      "outputs": [],
      "source": [
        "tr = spark.createDataFrame(train)\n",
        "te = spark.createDataFrame(test)\n",
        "try:\n",
        "  tr.union(te).show()\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4ayuEZI5aSS"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import lit\n",
        "tr.union(te.withColumn('y', lit(None))).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RTiumkR5aSS"
      },
      "outputs": [],
      "source": [
        "te.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqCEN1jh5aSS"
      },
      "outputs": [],
      "source": [
        "te.select(te.x1, lit(None), 'x2', col('x3')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2P14otn5aSS"
      },
      "outputs": [],
      "source": [
        "tr.union(te.select(te.x1, lit(None), 'x2', col('x3'))).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNxM9XKt5aSS"
      },
      "outputs": [],
      "source": [
        "tr.drop('y').union(te).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxmwQG7S5aSS"
      },
      "outputs": [],
      "source": [
        "tr.drop('y').union(te).explain(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvGStjzP5aSS"
      },
      "source": [
        "## Join"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JAwia7g5aST"
      },
      "outputs": [],
      "source": [
        "user = pd.DataFrame({\"name\": [\"admin\", \"guest\"],\n",
        "                      \"id\": [1, 123]})\n",
        "user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Mb_FYsW5aST"
      },
      "outputs": [],
      "source": [
        "n = 10\n",
        "log = pd.DataFrame({\"uid\": np.random.choice([1, 123], n),\n",
        "                    \"result\": np.random.choice([\"done\", \"error\"], n),\n",
        "                    \"time\": np.arange(n)})\n",
        "log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhUNR1O_5aST"
      },
      "outputs": [],
      "source": [
        "log.join(user.set_index('id'), on='uid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGJtXV2z5aST"
      },
      "outputs": [],
      "source": [
        "users = spark.createDataFrame(user)\n",
        "logs = spark.createDataFrame(log)\n",
        "users.withColumnRenamed('id', 'uid').join(logs, 'uid').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o_jOWvm5aST"
      },
      "outputs": [],
      "source": [
        "users.join(logs, users.id==logs.uid).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlGYmXhs5aST"
      },
      "source": [
        "## UDF - user-defined functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ywa4zmyn5aST"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import pandas_udf\n",
        "\n",
        "@pandas_udf('bigint')\n",
        "def pandas_plus_one(series: pd.Series) -> pd.Series:\n",
        "    return series + 1\n",
        "\n",
        "df.select(pandas_plus_one(df.a)).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jY7LbxR5aST"
      },
      "outputs": [],
      "source": [
        "[pandas_plus_one(col(c)) for c in df.columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNI_y-eS5aST"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StringType\n",
        "StringType.typeName()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qFIHMI95aST"
      },
      "outputs": [],
      "source": [
        "@pandas_udf('string')\n",
        "def pandas_to_str(series: pd.Series) -> pd.Series:\n",
        "    return series.astype('str').str.pad(10, 'left', '_')\n",
        "\n",
        "df.select(*[pandas_to_str(col(c)) for c in df.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DEkIJvC5aSU"
      },
      "outputs": [],
      "source": [
        "def pandas_filter_func(iterator):\n",
        "    for pandas_df in iterator:\n",
        "        yield pandas_df[pandas_df.a == 1]\n",
        "\n",
        "df.mapInPandas(pandas_filter_func, schema=df.schema).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx_busqd5aSU"
      },
      "source": [
        "## Группировка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVcV82K95aSU"
      },
      "outputs": [],
      "source": [
        "fruit = spark.createDataFrame([\n",
        "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
        "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
        "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n",
        "fruit.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfTmU7HH5aSU"
      },
      "outputs": [],
      "source": [
        "fruit.groupby('color').avg().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8BlDelB5aSU"
      },
      "outputs": [],
      "source": [
        "def plus_mean(pandas_df):\n",
        "    return pandas_df.assign(v1=pandas_df.v1 - pandas_df.v1.mean())\n",
        "\n",
        "fruit.groupby('color').applyInPandas(plus_mean, schema=fruit.schema).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heikKMmN5aSU"
      },
      "source": [
        "## SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZqAJkvE5aSU"
      },
      "outputs": [],
      "source": [
        "fruit.createOrReplaceTempView(\"tableA\")\n",
        "spark.sql(\"SELECT * from tableA\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFhm8_LC5aSU"
      },
      "outputs": [],
      "source": [
        "spark.sql(\"SELECT * from tableA\").explain()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "St6nCDRA5aSU"
      },
      "outputs": [],
      "source": [
        "print(spark.sql(\"EXPLAIN SELECT * from tableA\").collect()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcW5kmam5aSU"
      },
      "outputs": [],
      "source": [
        "spark.sql(\"SELECT count(*) from tableA\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WCjRDgE5aSU"
      },
      "outputs": [],
      "source": [
        "@pandas_udf(\"integer\")\n",
        "def add_one(s: pd.Series) -> pd.Series:\n",
        "    return s + 1\n",
        "\n",
        "spark.udf.register(\"add_one\", add_one)\n",
        "spark.sql(\"SELECT add_one(v1) FROM tableA\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B25AC-9C5aSU"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import expr\n",
        "\n",
        "fruit.selectExpr('add_one(v1)').show()\n",
        "fruit.select(expr('count(*)') > 0).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Mys21315aSV"
      },
      "source": [
        "# Ввод-вывод"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHDrnFpY5aSV"
      },
      "source": [
        "## Запись и чтение результата"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sn-weFxj5aSV"
      },
      "outputs": [],
      "source": [
        "rm -r foo.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQz-u9-l5aSV"
      },
      "outputs": [],
      "source": [
        "df.write.csv('foo.csv', header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZMOg4AT5aSV"
      },
      "outputs": [],
      "source": [
        "ls -lh foo.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieX0B2hH5aSV"
      },
      "outputs": [],
      "source": [
        "df.rdd.getNumPartitions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXyE7YgJ5aSV"
      },
      "outputs": [],
      "source": [
        "cat foo.csv/part-00001-*.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bKn7-oW5aSW"
      },
      "outputs": [],
      "source": [
        "spark.read.csv('foo.csv/part-00001*.csv', header=True).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIYbSsAJ5aSW"
      },
      "outputs": [],
      "source": [
        "spark.read.csv('foo.csv', header=True).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Akz-AdAZ5aSX"
      },
      "outputs": [],
      "source": [
        "!hd foo.csv/part-00001*.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqAoZ7625aSX"
      },
      "outputs": [],
      "source": [
        "  df.repartition(3).rdd.getNumPartitions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhPcKkuz5aSX"
      },
      "outputs": [],
      "source": [
        "rm -r bar.parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uG26YuFG5aSX"
      },
      "outputs": [],
      "source": [
        "df.repartition(3).write.parquet('bar.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tN9CuaRT5aSY"
      },
      "outputs": [],
      "source": [
        "ls -lh bar.parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ynoumVh5aSY"
      },
      "outputs": [],
      "source": [
        "!hd bar.parquet/part-00000-*.parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoEMYXik5aSY"
      },
      "outputs": [],
      "source": [
        "spark.read.parquet('bar.parquet/part-00000-*.parquet').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kACGQOYi5aSY"
      },
      "outputs": [],
      "source": [
        "spark.read.parquet('bar.parquet').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9E_bdvYG5aSY"
      },
      "source": [
        "## Текстовый формат"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gwl-bJbs5aSY"
      },
      "outputs": [],
      "source": [
        "textFile = spark.read.text(\"./spark-3.2.1-bin-hadoop3.2/README.md\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uoi_D_k5aSY"
      },
      "outputs": [],
      "source": [
        "textFile.repartition(4).explain()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKxTLqRi5aSY"
      },
      "outputs": [],
      "source": [
        "textFile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1llgyhw5aSY"
      },
      "outputs": [],
      "source": [
        "textFile.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBNzj3OU5aSY"
      },
      "outputs": [],
      "source": [
        "textFile.first()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBDVbtJC5aSY"
      },
      "outputs": [],
      "source": [
        "linesWithSpark = textFile.filter(textFile.value.contains(\"Spark\"))\n",
        "linesWithSpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCY3aYfa5aSY"
      },
      "outputs": [],
      "source": [
        "linesWithSpark.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1Ja5MID5aSY"
      },
      "outputs": [],
      "source": [
        "linesWithSpark.rdd.getNumPartitions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkTwv9cS5aSY"
      },
      "outputs": [],
      "source": [
        "rm -r spark.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gbffnor75aSZ"
      },
      "outputs": [],
      "source": [
        "linesWithSpark.write.text(\"spark.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGBKzhdk5aSZ"
      },
      "outputs": [],
      "source": [
        "ls -lh spark.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4K7Zr8h5aSZ"
      },
      "outputs": [],
      "source": [
        "!head spark.txt/part*.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qG1p6ieY5aSZ"
      },
      "outputs": [],
      "source": [
        "linesWithSpark.write.csv(\"spark.csv\", header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hsg8uzE5aSZ"
      },
      "outputs": [],
      "source": [
        "ls -lh spark.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4DmpKVu5aSZ"
      },
      "outputs": [],
      "source": [
        "!head spark.csv/part*.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAzgXUKb5aSZ"
      },
      "outputs": [],
      "source": [
        "rm -r spark.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc1PYQcz5aSZ"
      },
      "source": [
        "## Кеширование"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psp_aW4a5aSZ"
      },
      "outputs": [],
      "source": [
        "linesWithSpark.selectExpr(\"count(*)\").explain(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK_Rc-Yn5aSZ"
      },
      "source": [
        "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.StorageLevel.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmrQwFpB5aSZ"
      },
      "outputs": [],
      "source": [
        "linesWithSpark.storageLevel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVwM1CNH5aSZ"
      },
      "outputs": [],
      "source": [
        "linesWithSpark.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2jjefP65aSa"
      },
      "outputs": [],
      "source": [
        "linesWithSpark.storageLevel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFV4Mkr05aSa"
      },
      "outputs": [],
      "source": [
        "linesWithSpark.selectExpr(\"count(*)\").explain(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t75kSXSW5aSa"
      },
      "outputs": [],
      "source": [
        "linesWithSpark.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAH25aae5aSa"
      },
      "outputs": [],
      "source": [
        "linesWithSpark.selectExpr(\"count(*)\").explain(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y-x5ipT5aSa"
      },
      "source": [
        "# Реальный датасет"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGsFxZGr5aSa"
      },
      "outputs": [],
      "source": [
        "ch_train = spark.read.csv('/content/sample_data/california_housing_train.csv', header=True)\n",
        "ch_test = spark.read.csv('/content/sample_data/california_housing_test.csv', header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Dbj9iqi5aSa"
      },
      "outputs": [],
      "source": [
        "ch_train.count(), ch_test.count(), ch_train.count() + ch_test.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IqNxGzU5aSa"
      },
      "outputs": [],
      "source": [
        "ch = ch_train.union(ch_test)\n",
        "ch.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HRSVCp05aSa"
      },
      "outputs": [],
      "source": [
        "ch.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6p9YSyhZ5aSa"
      },
      "outputs": [],
      "source": [
        "ch.sample(False, 0.001, seed=0).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhyuwbp_5aSa"
      },
      "outputs": [],
      "source": [
        "ch.selectExpr(\"cast(longitude as float) longitude\",\n",
        "    \"cast(latitude as float) latitude\",\n",
        "    \"cast(housing_median_age as float) housing_median_age\",\n",
        "    \"cast(total_rooms as float) total_rooms\",\n",
        "    \"cast(total_bedrooms as float) total_bedrooms\",\n",
        "    \"cast(population as float) population\",\n",
        "    \"cast(households as float) households\",\n",
        "    \"cast(median_income as float) median_income\",\n",
        "    \"cast(median_house_value as float) median_house_value\").printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGkunyxp5aSa"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "ch.select(*[ch[c].cast(FloatType()) for c in ch.columns]).printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eR16QaZ5aSa"
      },
      "outputs": [],
      "source": [
        "ch_train_location = \"sample_data/california_housing_train.csv\"\n",
        "ch_test_location = \"sample_data/california_housing_test.csv\"\n",
        "\n",
        "reader = spark.read.format(\"csv\") \\\n",
        "  .option(\"inferSchema\", True) \\\n",
        "  .option(\"header\", True) \\\n",
        "  .option(\"sep\", \",\")\n",
        "  \n",
        "ch_train = reader.load(ch_train_location)\n",
        "ch_test = reader.load(ch_test_location)\n",
        " \n",
        "ch_train.show(3)\n",
        "ch_test.show(3, truncate=False)\n",
        "ch_test.show(1, vertical=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkdY1XR25aSa"
      },
      "outputs": [],
      "source": [
        "ch_train.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxtYQsoi5aSb"
      },
      "outputs": [],
      "source": [
        "ch.describe().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrGiAav95aSb"
      },
      "source": [
        "## Добавим признаков"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfwe2G4Y5aSb"
      },
      "outputs": [],
      "source": [
        "ch = ch.withColumn(\"median_house_value\", ch[\"median_house_value\"]/100000)\n",
        "ch.take(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HYzrcIZ5aSb"
      },
      "outputs": [],
      "source": [
        "ch[\"total_rooms\"]/ch[\"households\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQ-bXZCA5aSb"
      },
      "outputs": [],
      "source": [
        "roomsPerHousehold = ch.select(ch[\"total_rooms\"]/ch[\"households\"])\n",
        "roomsPerHousehold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZnNE4Re5aSb"
      },
      "outputs": [],
      "source": [
        "ch = ch.withColumn('rooms_per_household', ch[\"total_rooms\"]/ch[\"households\"])\n",
        "ch.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6uixcbm5aSb"
      },
      "outputs": [],
      "source": [
        "ch = ch.withColumn('population_per_household', ch[\"population\"]/ch[\"households\"])\n",
        "ch.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qp6n-HpT5aSb"
      },
      "outputs": [],
      "source": [
        "ch = ch.withColumn('bedrooms_per_room', ch[\"total_bedrooms\"]/ch[\"total_rooms\"])\n",
        "ch.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJoA69Oh5aSb"
      },
      "outputs": [],
      "source": [
        "ch = ch.select(\"median_house_value\", \n",
        "              \"total_bedrooms\", \n",
        "              \"population\", \n",
        "              \"households\", \n",
        "              \"median_income\", \n",
        "              \"rooms_per_household\", \n",
        "              \"population_per_household\", \n",
        "              \"bedrooms_per_room\")\n",
        "\n",
        "ch.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF3XIdJ-5aSb"
      },
      "source": [
        "# Задание\n",
        "1. Прочитать датасет пассажиров титаника в спарк\n",
        "2. Вывести pie chart по количеству пассажиров в разных классах с помощью tempView и SQL\n",
        "3. Найти среднюю выживаемость пассажиров по полу и классу с помощью tempView и SQL\n",
        "4. Вывести pie chart по количеству пассажиров в разных классах методами спарка без tempView\n",
        "5. Найти среднюю выживаемость пассажиров по полу и классу методами спарка без tempView"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmBOYq5f5aSc"
      },
      "outputs": [],
      "source": [
        "!wget -c https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZjSSsLD5aSc"
      },
      "outputs": [],
      "source": [
        "!head titanic.csv"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "COPY_10.ipynb\"",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}